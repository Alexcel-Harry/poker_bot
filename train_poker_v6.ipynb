{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ° Deep CFR Poker - Ultra-Optimized (v6.0)\n",
    "\n",
    "## Fixes from v5.0\n",
    "- âœ… **Fixed tensor creation** - Pre-convert to numpy arrays (no more slow warning)\n",
    "- âœ… **Reduced training overhead** - Fewer steps, more traversals\n",
    "- âœ… **Simplified sampling** - Uniform sampling (linear weighting in targets instead)\n",
    "- âœ… **Vectorized operations** - Batch everything possible\n",
    "- âœ… **Full network capacity** - [256, 256, 128] preserved\n",
    "\n",
    "**Target: Complete 2M+ traversals in <10 hours**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q rlcard torch\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"DEEP CFR POKER - ULTRA-OPTIMIZED v6.0\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.cuda.get_device_name(0)\n",
    "    mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu} ({mem:.1f} GB)\")\n",
    "else:\n",
    "    print(\"âš ï¸ NO GPU - Training will be slow!\")\n",
    "print(\"=\" * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create Optimized Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_cfr_v6.py\n",
    "\"\"\"Deep CFR Ultra-Optimized v6.0\n",
    "\n",
    "Key optimizations:\n",
    "1. Pre-allocated numpy arrays for all batch operations\n",
    "2. Reduced training steps (quality over quantity)\n",
    "3. Circular buffer with O(1) operations\n",
    "4. Vectorized regret matching\n",
    "5. Efficient GPU memory usage\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import rlcard\n",
    "from rlcard.utils import set_seed\n",
    "\n",
    "# ============== CONFIGURATION ==============\n",
    "NUM_PLAYERS = 3\n",
    "NUM_TRAVERSALS = 3_000_000      # Total traversals (not iterations)\n",
    "TRAVERSALS_PER_BATCH = 512      # Traversals before each training step\n",
    "EVAL_EVERY = 100_000            # Evaluate every N traversals\n",
    "SAVE_EVERY = 500_000\n",
    "LOG_EVERY = 20_000\n",
    "SAVE_PATH = 'poker_deep_cfr'\n",
    "SEED = 42\n",
    "\n",
    "# Network - FULL CAPACITY\n",
    "HIDDEN_LAYERS = [256, 256, 128]\n",
    "\n",
    "# Training - OPTIMIZED\n",
    "BATCH_SIZE = 1024               # Large batch for GPU efficiency\n",
    "BUFFER_SIZE = 300_000           # Per player (900K total)\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_STEPS = 50                # Reduced from 200 - quality over quantity\n",
    "GRAD_CLIP = 1.0\n",
    "\n",
    "# Resume\n",
    "RESUME_FROM = 0\n",
    "\n",
    "# Time limit\n",
    "TIME_LIMIT = 11.0 * 3600\n",
    "TIME_BUFFER = 300\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"DEEP CFR POKER - ULTRA-OPTIMIZED v6.0\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"Players: {NUM_PLAYERS} | Target traversals: {NUM_TRAVERSALS:,}\")\n",
    "print(f\"Batch size: {TRAVERSALS_PER_BATCH} traversals, then {TRAIN_STEPS} train steps\")\n",
    "print(f\"Network: {HIDDEN_LAYERS} | Buffer/player: {BUFFER_SIZE:,}\")\n",
    "print(f\"Time limit: {TIME_LIMIT/3600:.1f}h\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "\n",
    "# ============== NEURAL NETWORKS ==============\n",
    "\n",
    "class AdvantageNet(nn.Module):\n",
    "    \"\"\"Predicts cumulative regrets/advantages\"\"\"\n",
    "    def __init__(self, state_dim, num_actions, hidden):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden[0])\n",
    "        self.ln1 = nn.LayerNorm(hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.ln2 = nn.LayerNorm(hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], hidden[2])\n",
    "        self.ln3 = nn.LayerNorm(hidden[2])\n",
    "        self.out = nn.Linear(hidden[2], num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.ln1(self.fc1(x)))\n",
    "        x = F.relu(self.ln2(self.fc2(x)))\n",
    "        x = F.relu(self.ln3(self.fc3(x)))\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class StrategyNet(nn.Module):\n",
    "    \"\"\"Outputs action probabilities\"\"\"\n",
    "    def __init__(self, state_dim, num_actions, hidden):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden[0])\n",
    "        self.ln1 = nn.LayerNorm(hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.ln2 = nn.LayerNorm(hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], hidden[2])\n",
    "        self.ln3 = nn.LayerNorm(hidden[2])\n",
    "        self.out = nn.Linear(hidden[2], num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.ln1(self.fc1(x)))\n",
    "        x = F.relu(self.ln2(self.fc2(x)))\n",
    "        x = F.relu(self.ln3(self.fc3(x)))\n",
    "        return F.softmax(self.out(x), dim=-1)\n",
    "\n",
    "\n",
    "# ============== FAST CIRCULAR BUFFER ==============\n",
    "\n",
    "class FastBuffer:\n",
    "    \"\"\"O(1) circular buffer with pre-allocated arrays\"\"\"\n",
    "    __slots__ = ['capacity', 'states', 'targets', 'masks', 'pos', 'size', 'full']\n",
    "    \n",
    "    def __init__(self, capacity, state_dim, num_actions):\n",
    "        self.capacity = capacity\n",
    "        self.states = np.zeros((capacity, state_dim), dtype=np.float32)\n",
    "        self.targets = np.zeros((capacity, num_actions), dtype=np.float32)\n",
    "        self.masks = np.zeros((capacity, num_actions), dtype=np.float32)\n",
    "        self.pos = 0\n",
    "        self.size = 0\n",
    "        self.full = False\n",
    "    \n",
    "    def add(self, state, target, mask):\n",
    "        self.states[self.pos] = state\n",
    "        self.targets[self.pos] = target\n",
    "        self.masks[self.pos] = mask\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "        if not self.full:\n",
    "            self.size += 1\n",
    "            if self.size == self.capacity:\n",
    "                self.full = True\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        actual = min(batch_size, self.size)\n",
    "        idx = np.random.randint(0, self.size, actual)\n",
    "        return self.states[idx], self.targets[idx], self.masks[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "# ============== DEEP CFR SOLVER ==============\n",
    "\n",
    "class DeepCFR:\n",
    "    def __init__(self, state_dim, num_actions, device):\n",
    "        self.state_dim = state_dim\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "        \n",
    "        # Networks\n",
    "        self.adv_nets = [AdvantageNet(state_dim, num_actions, HIDDEN_LAYERS).to(device) \n",
    "                         for _ in range(NUM_PLAYERS)]\n",
    "        self.strat_nets = [StrategyNet(state_dim, num_actions, HIDDEN_LAYERS).to(device)\n",
    "                           for _ in range(NUM_PLAYERS)]\n",
    "        \n",
    "        # Optimizers\n",
    "        self.adv_opts = [optim.Adam(net.parameters(), lr=LEARNING_RATE) \n",
    "                         for net in self.adv_nets]\n",
    "        self.strat_opts = [optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "                           for net in self.strat_nets]\n",
    "        \n",
    "        # Buffers\n",
    "        self.adv_bufs = [FastBuffer(BUFFER_SIZE, state_dim, num_actions)\n",
    "                         for _ in range(NUM_PLAYERS)]\n",
    "        self.strat_bufs = [FastBuffer(BUFFER_SIZE, state_dim, num_actions)\n",
    "                           for _ in range(NUM_PLAYERS)]\n",
    "        \n",
    "        # Set to eval mode by default\n",
    "        for net in self.adv_nets + self.strat_nets:\n",
    "            net.eval()\n",
    "        \n",
    "        self.traversals = 0\n",
    "        self.iteration = 0  # Training iterations\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_strategy_batch(self, obs_array, player, legal_batch):\n",
    "        \"\"\"Get strategies for batch - FIXED: uses numpy array not list\"\"\"\n",
    "        # obs_array should already be np.ndarray\n",
    "        obs_t = torch.from_numpy(obs_array).to(self.device)\n",
    "        advantages = self.adv_nets[player](obs_t).cpu().numpy()\n",
    "        \n",
    "        batch_size = len(obs_array)\n",
    "        strategies = np.zeros((batch_size, self.num_actions), dtype=np.float32)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            legal = legal_batch[i]\n",
    "            if not legal:\n",
    "                continue\n",
    "            \n",
    "            # Regret matching: positive advantages only\n",
    "            adv = advantages[i]\n",
    "            pos_adv = np.maximum(adv[legal], 0)\n",
    "            total = pos_adv.sum()\n",
    "            \n",
    "            if total > 1e-8:\n",
    "                strategies[i, legal] = pos_adv / total\n",
    "            else:\n",
    "                strategies[i, legal] = 1.0 / len(legal)\n",
    "        \n",
    "        return strategies\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_strategy_single(self, obs, player, legal):\n",
    "        \"\"\"Get strategy for single state\"\"\"\n",
    "        obs_t = torch.from_numpy(obs.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "        adv = self.adv_nets[player](obs_t).cpu().numpy()[0]\n",
    "        \n",
    "        strategy = np.zeros(self.num_actions, dtype=np.float32)\n",
    "        pos_adv = np.maximum(adv[legal], 0)\n",
    "        total = pos_adv.sum()\n",
    "        \n",
    "        if total > 1e-8:\n",
    "            strategy[legal] = pos_adv / total\n",
    "        else:\n",
    "            strategy[legal] = 1.0 / len(legal)\n",
    "        \n",
    "        return strategy\n",
    "    \n",
    "    def traverse(self, env, traverser):\n",
    "        \"\"\"Single external-sampling traversal\"\"\"\n",
    "        state, player = env.reset()\n",
    "        history = []  # [(player, obs, legal, action, strategy)]\n",
    "        \n",
    "        while not env.is_over():\n",
    "            obs = state['obs']\n",
    "            legal = list(state.get('legal_actions', {}).keys())\n",
    "            if not legal:\n",
    "                break\n",
    "            \n",
    "            strategy = self.get_strategy_single(obs, player, legal)\n",
    "            \n",
    "            # Sample action\n",
    "            probs = strategy[legal]\n",
    "            action = np.random.choice(legal, p=probs/probs.sum())\n",
    "            \n",
    "            history.append((player, obs.copy(), list(legal), action, strategy.copy()))\n",
    "            state, player = env.step(action)\n",
    "        \n",
    "        # Process trajectory\n",
    "        payoffs = env.get_payoffs()\n",
    "        self._update_buffers(history, payoffs, traverser)\n",
    "        self.traversals += 1\n",
    "        \n",
    "        return payoffs[traverser]\n",
    "    \n",
    "    def _update_buffers(self, history, payoffs, traverser):\n",
    "        \"\"\"Update advantage and strategy buffers\"\"\"\n",
    "        # Linear CFR weight (applied to targets, not sampling)\n",
    "        weight = self.iteration + 1\n",
    "        \n",
    "        for player, obs, legal, action, strategy in history:\n",
    "            mask = np.zeros(self.num_actions, dtype=np.float32)\n",
    "            mask[legal] = 1.0\n",
    "            \n",
    "            if player == traverser:\n",
    "                # Advantage/regret estimation\n",
    "                # For external sampling: we only know the value of sampled action\n",
    "                # Use importance-weighted estimate\n",
    "                regrets = np.zeros(self.num_actions, dtype=np.float32)\n",
    "                value = payoffs[traverser]\n",
    "                \n",
    "                # Sampled action got this value; estimate others as slightly less\n",
    "                # This encourages exploration of the action that led to good outcomes\n",
    "                for a in legal:\n",
    "                    if a == action:\n",
    "                        regrets[a] = value * weight\n",
    "                    else:\n",
    "                        # Conservative estimate for unsampled actions\n",
    "                        regrets[a] = value * weight * strategy[action]\n",
    "                \n",
    "                self.adv_bufs[traverser].add(obs, regrets, mask)\n",
    "            \n",
    "            # Always update strategy buffer (average strategy)\n",
    "            weighted_strat = strategy * weight\n",
    "            self.strat_bufs[player].add(obs, weighted_strat, mask)\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"One training step for all networks\"\"\"\n",
    "        self.iteration += 1\n",
    "        total_loss = 0\n",
    "        \n",
    "        for p in range(NUM_PLAYERS):\n",
    "            # Train advantage network\n",
    "            if len(self.adv_bufs[p]) >= BATCH_SIZE:\n",
    "                self.adv_nets[p].train()\n",
    "                for _ in range(TRAIN_STEPS):\n",
    "                    states, targets, masks = self.adv_bufs[p].sample(BATCH_SIZE)\n",
    "                    \n",
    "                    states_t = torch.from_numpy(states).to(self.device)\n",
    "                    targets_t = torch.from_numpy(targets).to(self.device)\n",
    "                    masks_t = torch.from_numpy(masks).to(self.device)\n",
    "                    \n",
    "                    pred = self.adv_nets[p](states_t)\n",
    "                    loss = ((pred - targets_t) ** 2 * masks_t).sum() / masks_t.sum().clamp(min=1)\n",
    "                    \n",
    "                    self.adv_opts[p].zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.adv_nets[p].parameters(), GRAD_CLIP)\n",
    "                    self.adv_opts[p].step()\n",
    "                    total_loss += loss.item()\n",
    "                self.adv_nets[p].eval()\n",
    "            \n",
    "            # Train strategy network\n",
    "            if len(self.strat_bufs[p]) >= BATCH_SIZE:\n",
    "                self.strat_nets[p].train()\n",
    "                for _ in range(TRAIN_STEPS // 2):\n",
    "                    states, targets, masks = self.strat_bufs[p].sample(BATCH_SIZE)\n",
    "                    \n",
    "                    states_t = torch.from_numpy(states).to(self.device)\n",
    "                    targets_t = torch.from_numpy(targets).to(self.device)\n",
    "                    masks_t = torch.from_numpy(masks).to(self.device)\n",
    "                    \n",
    "                    # Normalize targets\n",
    "                    targets_sum = (targets_t * masks_t).sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "                    targets_norm = targets_t / targets_sum\n",
    "                    \n",
    "                    pred = self.strat_nets[p](states_t)\n",
    "                    loss = -(targets_norm * (pred + 1e-8).log() * masks_t).sum() / masks_t.sum().clamp(min=1)\n",
    "                    \n",
    "                    self.strat_opts[p].zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.strat_nets[p].parameters(), GRAD_CLIP)\n",
    "                    self.strat_opts[p].step()\n",
    "                self.strat_nets[p].eval()\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def evaluate(self, env, num_games=500):\n",
    "        \"\"\"Evaluate using strategy networks\"\"\"\n",
    "        rewards = np.zeros(NUM_PLAYERS)\n",
    "        \n",
    "        for _ in range(num_games):\n",
    "            state, player = env.reset()\n",
    "            while not env.is_over():\n",
    "                obs = state['obs']\n",
    "                legal = list(state.get('legal_actions', {}).keys())\n",
    "                if not legal:\n",
    "                    break\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    obs_t = torch.from_numpy(obs.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "                    probs = self.strat_nets[player](obs_t).cpu().numpy()[0]\n",
    "                \n",
    "                probs_legal = probs[legal]\n",
    "                probs_legal = probs_legal / probs_legal.sum()\n",
    "                action = np.random.choice(legal, p=probs_legal)\n",
    "                state, player = env.step(action)\n",
    "            \n",
    "            rewards += env.get_payoffs()\n",
    "        \n",
    "        return rewards / num_games\n",
    "    \n",
    "    def save(self, path, tag=''):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        fname = f'model_{tag}.pt' if tag else 'model_latest.pt'\n",
    "        \n",
    "        torch.save({\n",
    "            'traversals': self.traversals,\n",
    "            'iteration': self.iteration,\n",
    "            'adv_nets': [net.state_dict() for net in self.adv_nets],\n",
    "            'strat_nets': [net.state_dict() for net in self.strat_nets],\n",
    "            'adv_opts': [opt.state_dict() for opt in self.adv_opts],\n",
    "            'strat_opts': [opt.state_dict() for opt in self.strat_opts],\n",
    "            'config': {\n",
    "                'num_players': NUM_PLAYERS,\n",
    "                'state_dim': self.state_dim,\n",
    "                'num_actions': self.num_actions,\n",
    "                'hidden_layers': HIDDEN_LAYERS,\n",
    "            }\n",
    "        }, os.path.join(path, fname))\n",
    "        \n",
    "        # Also save strategy-only for inference\n",
    "        torch.save({\n",
    "            'num_players': NUM_PLAYERS,\n",
    "            'state_dim': self.state_dim,\n",
    "            'num_actions': self.num_actions,\n",
    "            'hidden_layers': HIDDEN_LAYERS,\n",
    "            'strat_nets': [net.state_dict() for net in self.strat_nets],\n",
    "        }, os.path.join(path, 'strategy_only.pt'))\n",
    "        \n",
    "        print(f\"ðŸ’¾ Saved: {path}/{fname}\")\n",
    "    \n",
    "    def load(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            return False\n",
    "        ckpt = torch.load(path, map_location=self.device)\n",
    "        self.traversals = ckpt['traversals']\n",
    "        self.iteration = ckpt['iteration']\n",
    "        for i, sd in enumerate(ckpt['adv_nets']):\n",
    "            self.adv_nets[i].load_state_dict(sd)\n",
    "        for i, sd in enumerate(ckpt['strat_nets']):\n",
    "            self.strat_nets[i].load_state_dict(sd)\n",
    "        for i, sd in enumerate(ckpt['adv_opts']):\n",
    "            self.adv_opts[i].load_state_dict(sd)\n",
    "        for i, sd in enumerate(ckpt['strat_opts']):\n",
    "            self.strat_opts[i].load_state_dict(sd)\n",
    "        print(f\"âœ“ Loaded: {self.traversals:,} traversals, iter {self.iteration}\")\n",
    "        return True\n",
    "\n",
    "\n",
    "# ============== MAIN TRAINING LOOP ==============\n",
    "\n",
    "def train():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nðŸ”§ Device: {device}\")\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Seeds\n",
    "    set_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    # Create environments (pool for traversals)\n",
    "    print(f\"\\nðŸŽ® Creating environment pool...\")\n",
    "    cfg = {'seed': SEED, 'game_num_players': NUM_PLAYERS}\n",
    "    env_pool = [rlcard.make('no-limit-holdem', config={'seed': SEED + i, 'game_num_players': NUM_PLAYERS})\n",
    "                for i in range(32)]  # Pool of envs for parallel-ish traversals\n",
    "    eval_env = rlcard.make('no-limit-holdem', config=cfg)\n",
    "    \n",
    "    state_dim = env_pool[0].state_shape[0][0]\n",
    "    num_actions = env_pool[0].num_actions\n",
    "    print(f\"   State dim: {state_dim}, Actions: {num_actions}\")\n",
    "    \n",
    "    # Create solver\n",
    "    print(f\"\\nðŸ¤– Creating Deep CFR solver...\")\n",
    "    solver = DeepCFR(state_dim, num_actions, device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for net in solver.adv_nets for p in net.parameters())\n",
    "    total_params += sum(p.numel() for net in solver.strat_nets for p in net.parameters())\n",
    "    print(f\"   Parameters: {total_params:,}\")\n",
    "    \n",
    "    # Resume\n",
    "    if RESUME_FROM > 0:\n",
    "        solver.load(f\"{SAVE_PATH}/model_trav{RESUME_FROM}.pt\")\n",
    "    \n",
    "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "    best_reward = -float('inf')\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 65)\n",
    "    print(\"TRAINING STARTED\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    last_log = 0\n",
    "    last_eval = 0\n",
    "    last_save = 0\n",
    "    env_idx = 0\n",
    "    \n",
    "    while solver.traversals < NUM_TRAVERSALS:\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        # Run traversals\n",
    "        for _ in range(TRAVERSALS_PER_BATCH):\n",
    "            traverser = solver.traversals % NUM_PLAYERS\n",
    "            env = env_pool[env_idx]\n",
    "            env_idx = (env_idx + 1) % len(env_pool)\n",
    "            solver.traverse(env, traverser)\n",
    "        \n",
    "        # Train\n",
    "        solver.train_step()\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        trav_per_sec = TRAVERSALS_PER_BATCH / batch_time\n",
    "        \n",
    "        # Logging\n",
    "        if solver.traversals - last_log >= LOG_EVERY:\n",
    "            last_log = solver.traversals\n",
    "            elapsed = time.time() - t0\n",
    "            remaining = TIME_LIMIT - elapsed\n",
    "            time_str = f\"{remaining/3600:.1f}h\" if remaining > 0 else \"OVER!\"\n",
    "            \n",
    "            eta = (NUM_TRAVERSALS - solver.traversals) / trav_per_sec / 3600 if trav_per_sec > 0 else 99\n",
    "            \n",
    "            buf_size = sum(len(b) for b in solver.adv_bufs)\n",
    "            pct = solver.traversals / NUM_TRAVERSALS * 100\n",
    "            \n",
    "            print(f\"Trav: {solver.traversals:>9,}/{NUM_TRAVERSALS:,} ({pct:5.1f}%) | \"\n",
    "                  f\"{trav_per_sec:>5.0f}/s | Buf: {buf_size:,} | \"\n",
    "                  f\"ETA: {eta:.1f}h | Left: {time_str}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        if solver.traversals - last_eval >= EVAL_EVERY:\n",
    "            last_eval = solver.traversals\n",
    "            rewards = solver.evaluate(eval_env, 500)\n",
    "            print(f\"   ðŸ“Š Eval: \" + \" | \".join([f\"P{i}={rewards[i]:+.2f}\" for i in range(NUM_PLAYERS)]))\n",
    "            \n",
    "            if rewards[0] > best_reward:\n",
    "                best_reward = rewards[0]\n",
    "                print(f\"   ðŸ† New best: {best_reward:.2f}\")\n",
    "                solver.save(SAVE_PATH, 'best')\n",
    "        \n",
    "        # Checkpoint\n",
    "        if solver.traversals - last_save >= SAVE_EVERY:\n",
    "            last_save = solver.traversals\n",
    "            solver.save(SAVE_PATH, f'trav{solver.traversals}')\n",
    "        \n",
    "        # Time check\n",
    "        if TIME_LIMIT - (time.time() - t0) <= TIME_BUFFER:\n",
    "            print(\"\\n\" + \"=\" * 65)\n",
    "            print(\"â° TIME LIMIT - SAVING...\")\n",
    "            print(\"=\" * 65)\n",
    "            solver.save(SAVE_PATH, f'trav{solver.traversals}')\n",
    "            with open(f\"{SAVE_PATH}/resume.txt\", 'w') as f:\n",
    "                f.write(f\"RESUME_FROM = {solver.traversals}\\n\")\n",
    "                f.write(f\"# Iterations: {solver.iteration}\\n\")\n",
    "                f.write(f\"# Best reward: {best_reward:.4f}\\n\")\n",
    "            print(f\"\\nTo resume: Set RESUME_FROM = {solver.traversals}\")\n",
    "            return 'timeout', solver.traversals, best_reward\n",
    "    \n",
    "    # Final save\n",
    "    solver.save(SAVE_PATH, 'final')\n",
    "    \n",
    "    total_time = time.time() - t0\n",
    "    print(\"\\n\" + \"=\" * 65)\n",
    "    print(\"âœ… TRAINING COMPLETE!\")\n",
    "    print(\"=\" * 65)\n",
    "    print(f\"   Time: {total_time/3600:.2f} hours\")\n",
    "    print(f\"   Traversals: {solver.traversals:,}\")\n",
    "    print(f\"   Training iterations: {solver.iteration:,}\")\n",
    "    print(f\"   Best reward: {best_reward:.2f}\")\n",
    "    print(f\"   Model saved to: {SAVE_PATH}/\")\n",
    "    \n",
    "    return 'complete', solver.traversals, best_reward\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_cfr_v6.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "model_dir = 'poker_deep_cfr'\n",
    "\n",
    "if os.path.exists(model_dir):\n",
    "    print(\"Model files:\")\n",
    "    for f in sorted(os.listdir(model_dir)):\n",
    "        size = os.path.getsize(os.path.join(model_dir, f)) / 1e6\n",
    "        print(f\"  {f} ({size:.1f} MB)\")\n",
    "    \n",
    "    shutil.make_archive(model_dir, 'zip', model_dir)\n",
    "    print(f\"\\nâœ… Created {model_dir}.zip\")\n",
    "    \n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(f'{model_dir}.zip')\n",
    "    except:\n",
    "        print(f\"Download {model_dir}.zip from file browser\")\n",
    "else:\n",
    "    print(\"No model found - run training first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Resume Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_TRAV = 0  # <-- Set this from resume.txt\n",
    "\n",
    "if RESUME_TRAV > 0:\n",
    "    import re\n",
    "    with open('train_cfr_v6.py', 'r') as f:\n",
    "        code = f.read()\n",
    "    code = re.sub(r'RESUME_FROM = \\d+', f'RESUME_FROM = {RESUME_TRAV}', code)\n",
    "    with open('train_cfr_v6.py', 'w') as f:\n",
    "        f.write(code)\n",
    "    print(f\"âœ… Set RESUME_FROM = {RESUME_TRAV}\")\n",
    "    print(\"Now run Step 3 again!\")\n",
    "else:\n",
    "    import os\n",
    "    if os.path.exists('poker_deep_cfr/resume.txt'):\n",
    "        print(\"Found resume.txt:\")\n",
    "        print(open('poker_deep_cfr/resume.txt').read())\n",
    "    else:\n",
    "        print(\"No resume needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸ“‹ What Changed in v6.0\n",
    "\n",
    "| Issue in v5.0 | Fix in v6.0 |\n",
    "|--------------|-------------|\n",
    "| Tensor from list (slow warning) | `torch.from_numpy()` - proper pre-conversion |\n",
    "| 200+100 train steps/iter | 50+25 steps (less overhead) |\n",
    "| Linear-weighted sampling | Simple uniform sampling, weights in targets |\n",
    "| Buffer count logic | True circular buffer with `__slots__` |\n",
    "| Iteration-based progress | Traversal-based (clearer metric) |\n",
    "\n",
    "## Expected Performance\n",
    "\n",
    "| Metric | v5.0 | v6.0 Expected |\n",
    "|--------|------|---------------|\n",
    "| Speed (start) | 114/s | ~200-300/s |\n",
    "| Speed (end) | 50/s | ~150-200/s (stable) |\n",
    "| Time for 3M trav | 16-30h | **~6-10h** |\n",
    "\n",
    "## Training Quality\n",
    "\n",
    "Deep CFR convergence takes 2-5M traversals typically. With this setup:\n",
    "- 3M traversals in ~8-10 hours\n",
    "- Should see rewards stabilizing toward 0 (Nash equilibrium)\n",
    "- Can resume across sessions if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸŽ® Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile poker_agent.py\n",
    "\"\"\"Poker Agent for inference\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class StrategyNet(nn.Module):\n",
    "    def __init__(self, state_dim, num_actions, hidden):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden[0])\n",
    "        self.ln1 = nn.LayerNorm(hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.ln2 = nn.LayerNorm(hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], hidden[2])\n",
    "        self.ln3 = nn.LayerNorm(hidden[2])\n",
    "        self.out = nn.Linear(hidden[2], num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.ln1(self.fc1(x)))\n",
    "        x = F.relu(self.ln2(self.fc2(x)))\n",
    "        x = F.relu(self.ln3(self.fc3(x)))\n",
    "        return F.softmax(self.out(x), dim=-1)\n",
    "\n",
    "\n",
    "class PokerAgent:\n",
    "    def __init__(self, model_path, device='cpu'):\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        ckpt = torch.load(model_path, map_location=self.device)\n",
    "        self.num_players = ckpt['num_players']\n",
    "        self.state_dim = ckpt['state_dim']\n",
    "        self.num_actions = ckpt['num_actions']\n",
    "        hidden = ckpt['hidden_layers']\n",
    "        \n",
    "        self.nets = [StrategyNet(self.state_dim, self.num_actions, hidden).to(self.device)\n",
    "                     for _ in range(self.num_players)]\n",
    "        for i, sd in enumerate(ckpt['strat_nets']):\n",
    "            self.nets[i].load_state_dict(sd)\n",
    "            self.nets[i].eval()\n",
    "        \n",
    "        print(f\"Loaded {self.num_players}-player poker agent\")\n",
    "    \n",
    "    def act(self, state, player_id):\n",
    "        obs = state['obs']\n",
    "        legal = list(state.get('legal_actions', {}).keys())\n",
    "        if not legal:\n",
    "            return 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            obs_t = torch.from_numpy(obs.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "            probs = self.nets[player_id](obs_t).cpu().numpy()[0]\n",
    "        \n",
    "        probs_legal = probs[legal]\n",
    "        probs_legal = probs_legal / probs_legal.sum()\n",
    "        return np.random.choice(legal, p=probs_legal)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import rlcard\n",
    "    \n",
    "    agent = PokerAgent('poker_deep_cfr/strategy_only.pt')\n",
    "    env = rlcard.make('no-limit-holdem', config={'game_num_players': agent.num_players})\n",
    "    \n",
    "    state, player = env.reset()\n",
    "    while not env.is_over():\n",
    "        action = agent.act(state, player)\n",
    "        print(f\"P{player} -> action {action}\")\n",
    "        state, player = env.step(action)\n",
    "    \n",
    "    print(f\"Payoffs: {env.get_payoffs()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âœ… Inference code saved!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
